2026-02-01 11:42:59 Transformer(
  (embeddings): ModuleDict(
    (labs): Embedding(668, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (transformer): ModuleDict(
    (labs): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0-2): 3 x Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate='none')
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (fc): Linear(in_features=128, out_features=1, bias=True)
)
2026-02-01 11:42:59 Metrics: ['roc_auc', 'pr_auc', 'f1']
2026-02-01 11:42:59 Device: cuda
2026-02-01 11:42:59 
2026-02-01 11:42:59 Training:
2026-02-01 11:42:59 Batch size: 32
2026-02-01 11:42:59 Optimizer: <class 'torch.optim.adam.Adam'>
2026-02-01 11:42:59 Optimizer params: {'lr': 0.001}
2026-02-01 11:42:59 Weight decay: 0.0
2026-02-01 11:42:59 Max grad norm: None
2026-02-01 11:42:59 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x14f2c722a5f0>
2026-02-01 11:42:59 Monitor: None
2026-02-01 11:42:59 Monitor criterion: max
2026-02-01 11:42:59 Epochs: 5
2026-02-01 11:42:59 
2026-02-01 11:43:16 --- Train epoch-0, step-1426 ---
2026-02-01 11:43:16 loss: 0.5529
2026-02-01 11:43:18 --- Eval epoch-0, step-1426 ---
2026-02-01 11:43:18 roc_auc: 0.7483
2026-02-01 11:43:18 pr_auc: 0.2824
2026-02-01 11:43:18 f1: 0.1426
2026-02-01 11:43:18 loss: 0.2975
2026-02-01 11:43:18 
2026-02-01 11:43:29 --- Train epoch-1, step-2852 ---
2026-02-01 11:43:29 loss: 0.3147
2026-02-01 11:43:30 --- Eval epoch-1, step-2852 ---
2026-02-01 11:43:30 roc_auc: 0.7948
2026-02-01 11:43:30 pr_auc: 0.3106
2026-02-01 11:43:30 f1: 0.2396
2026-02-01 11:43:30 loss: 0.2817
2026-02-01 11:43:30 
2026-02-01 11:43:41 --- Train epoch-2, step-4278 ---
2026-02-01 11:43:41 loss: 0.3042
2026-02-01 11:43:41 --- Eval epoch-2, step-4278 ---
2026-02-01 11:43:41 roc_auc: 0.7862
2026-02-01 11:43:41 pr_auc: 0.3287
2026-02-01 11:43:41 f1: 0.2110
2026-02-01 11:43:41 loss: 0.2788
2026-02-01 11:43:41 
2026-02-01 11:43:52 --- Train epoch-3, step-5704 ---
2026-02-01 11:43:52 loss: 0.2930
2026-02-01 11:43:53 --- Eval epoch-3, step-5704 ---
2026-02-01 11:43:53 roc_auc: 0.7865
2026-02-01 11:43:53 pr_auc: 0.3293
2026-02-01 11:43:53 f1: 0.1552
2026-02-01 11:43:53 loss: 0.2772
2026-02-01 11:43:53 
2026-02-01 11:44:04 --- Train epoch-4, step-7130 ---
2026-02-01 11:44:04 loss: 0.2895
2026-02-01 11:44:05 --- Eval epoch-4, step-7130 ---
2026-02-01 11:44:05 roc_auc: 0.7999
2026-02-01 11:44:05 pr_auc: 0.3353
2026-02-01 11:44:05 f1: 0.2153
2026-02-01 11:44:05 loss: 0.2731
