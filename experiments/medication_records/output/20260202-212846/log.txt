2026-02-02 21:28:46 Transformer(
  (embeddings): ModuleDict(
    (labs): Embedding(669, 128, padding_idx=0)
    (dx): Embedding(3077, 128, padding_idx=0)
    (px): Embedding(657, 128, padding_idx=0)
    (demo): Embedding(8, 128, padding_idx=0)
  )
  (linear_layers): ModuleDict()
  (transformer): ModuleDict(
    (labs): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0-2): 3 x Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate='none')
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (dx): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0-2): 3 x Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate='none')
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (px): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0-2): 3 x Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate='none')
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (demo): TransformerLayer(
      (transformer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadedAttention(
            (linear_layers): ModuleList(
              (0-2): 3 x Linear(in_features=128, out_features=128, bias=False)
            )
            (output_linear): Linear(in_features=128, out_features=128, bias=False)
            (attention): Attention()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=128, out_features=512, bias=True)
            (w_2): Linear(in_features=512, out_features=128, bias=True)
            (dropout): Dropout(p=0.5, inplace=False)
            (activation): GELU(approximate='none')
          )
          (input_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (output_sublayer): SublayerConnection(
            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.5, inplace=False)
          )
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (fc): Linear(in_features=512, out_features=1, bias=True)
)
2026-02-02 21:28:46 Metrics: ['roc_auc', 'pr_auc', 'f1']
2026-02-02 21:28:46 Device: cuda
2026-02-02 21:28:46 
2026-02-02 21:28:46 Training:
2026-02-02 21:28:46 Batch size: 32
2026-02-02 21:28:46 Optimizer: <class 'torch.optim.adam.Adam'>
2026-02-02 21:28:46 Optimizer params: {'lr': 0.001}
2026-02-02 21:28:46 Weight decay: 0.0
2026-02-02 21:28:46 Max grad norm: None
2026-02-02 21:28:46 Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x14bcbba88a90>
2026-02-02 21:28:46 Monitor: None
2026-02-02 21:28:46 Monitor criterion: max
2026-02-02 21:28:46 Epochs: 10
2026-02-02 21:28:46 
2026-02-02 21:29:05 --- Train epoch-0, step-1474 ---
2026-02-02 21:29:05 loss: 0.2859
2026-02-02 21:29:05 --- Eval epoch-0, step-1474 ---
2026-02-02 21:29:05 roc_auc: 0.9307
2026-02-02 21:29:05 pr_auc: 0.9871
2026-02-02 21:29:05 f1: 0.9374
2026-02-02 21:29:05 loss: 0.2499
2026-02-02 21:29:05 
2026-02-02 21:29:23 --- Train epoch-1, step-2948 ---
2026-02-02 21:29:23 loss: 0.2396
2026-02-02 21:29:24 --- Eval epoch-1, step-2948 ---
2026-02-02 21:29:24 roc_auc: 0.9352
2026-02-02 21:29:24 pr_auc: 0.9881
2026-02-02 21:29:24 f1: 0.9385
2026-02-02 21:29:24 loss: 0.2553
2026-02-02 21:29:24 
2026-02-02 21:29:42 --- Train epoch-2, step-4422 ---
2026-02-02 21:29:42 loss: 0.2306
2026-02-02 21:29:43 --- Eval epoch-2, step-4422 ---
2026-02-02 21:29:43 roc_auc: 0.9379
2026-02-02 21:29:43 pr_auc: 0.9886
2026-02-02 21:29:43 f1: 0.9446
2026-02-02 21:29:43 loss: 0.2150
2026-02-02 21:29:43 
2026-02-02 21:30:01 --- Train epoch-3, step-5896 ---
2026-02-02 21:30:01 loss: 0.2209
2026-02-02 21:30:02 --- Eval epoch-3, step-5896 ---
2026-02-02 21:30:02 roc_auc: 0.9384
2026-02-02 21:30:02 pr_auc: 0.9886
2026-02-02 21:30:02 f1: 0.9473
2026-02-02 21:30:02 loss: 0.2173
2026-02-02 21:30:02 
2026-02-02 21:30:20 --- Train epoch-4, step-7370 ---
2026-02-02 21:30:20 loss: 0.2154
2026-02-02 21:30:21 --- Eval epoch-4, step-7370 ---
2026-02-02 21:30:21 roc_auc: 0.9352
2026-02-02 21:30:21 pr_auc: 0.9869
2026-02-02 21:30:21 f1: 0.9480
2026-02-02 21:30:21 loss: 0.2263
2026-02-02 21:30:21 
2026-02-02 21:30:39 --- Train epoch-5, step-8844 ---
2026-02-02 21:30:39 loss: 0.2114
2026-02-02 21:30:40 --- Eval epoch-5, step-8844 ---
2026-02-02 21:30:40 roc_auc: 0.9364
2026-02-02 21:30:40 pr_auc: 0.9878
2026-02-02 21:30:40 f1: 0.9455
2026-02-02 21:30:40 loss: 0.2312
2026-02-02 21:30:40 
2026-02-02 21:30:58 --- Train epoch-6, step-10318 ---
2026-02-02 21:30:58 loss: 0.2053
2026-02-02 21:30:59 --- Eval epoch-6, step-10318 ---
2026-02-02 21:30:59 roc_auc: 0.9328
2026-02-02 21:30:59 pr_auc: 0.9877
2026-02-02 21:30:59 f1: 0.9379
2026-02-02 21:30:59 loss: 0.2668
2026-02-02 21:30:59 
2026-02-02 21:31:17 --- Train epoch-7, step-11792 ---
2026-02-02 21:31:17 loss: 0.2036
2026-02-02 21:31:18 --- Eval epoch-7, step-11792 ---
2026-02-02 21:31:18 roc_auc: 0.9310
2026-02-02 21:31:18 pr_auc: 0.9853
2026-02-02 21:31:18 f1: 0.9478
2026-02-02 21:31:18 loss: 0.2271
2026-02-02 21:31:18 
2026-02-02 21:31:36 --- Train epoch-8, step-13266 ---
2026-02-02 21:31:36 loss: 0.1969
2026-02-02 21:31:36 --- Eval epoch-8, step-13266 ---
2026-02-02 21:31:36 roc_auc: 0.9340
2026-02-02 21:31:36 pr_auc: 0.9856
2026-02-02 21:31:36 f1: 0.9484
2026-02-02 21:31:36 loss: 0.2544
2026-02-02 21:31:36 
2026-02-02 21:31:55 --- Train epoch-9, step-14740 ---
2026-02-02 21:31:55 loss: 0.1965
2026-02-02 21:31:55 --- Eval epoch-9, step-14740 ---
2026-02-02 21:31:55 roc_auc: 0.9394
2026-02-02 21:31:55 pr_auc: 0.9885
2026-02-02 21:31:55 f1: 0.9485
2026-02-02 21:31:55 loss: 0.2245
